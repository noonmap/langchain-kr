{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01c423a8",
   "metadata": {},
   "source": [
    "## 환경설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a224fd32",
   "metadata": {},
   "source": [
    "API KEY 를 설정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "418ab505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024d0c5",
   "metadata": {},
   "source": [
    "LangChain으로 구축한 애플리케이션은 여러 단계에 걸쳐 LLM 호출을 여러 번 사용하게 됩니다. 이러한 애플리케이션이 점점 더 복잡해짐에 따라, 체인이나 에이전트 내부에서 정확히 무슨 일이 일어나고 있는지 조사할 수 있는 능력이 매우 중요해집니다. 이를 위한 최선의 방법은 [LangSmith](https://smith.langchain.com)를 사용하는 것입니다.\n",
    "\n",
    "LangSmith가 필수는 아니지만, 유용합니다. LangSmith를 사용하고 싶다면, 위의 링크에서 가입한 후, 로깅 추적을 시작하기 위해 환경 변수를 설정해야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edbbf89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain API Key가 설정되지 않았습니다. 참고: https://wikidocs.net/250954\n"
     ]
    }
   ],
   "source": [
    "# # LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# # !pip install -qU langchain-teddynote\n",
    "# from langchain_teddynote import logging\n",
    "\n",
    "# # 프로젝트 이름을 입력합니다.\n",
    "# logging.langsmith(\"CH12-RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de11a49",
   "metadata": {},
   "source": [
    "## 네이버 뉴스 기반 QA(Question-Answering) 챗봇\n",
    "\n",
    "이번 튜토리얼에는 네이버 뉴스기사의 내용에 대해 질문할 수 있는 **뉴스기사 QA 앱** 을 구축할 것입니다.\n",
    "\n",
    "이 가이드에서는 OpenAI 챗 모델과 임베딩, 그리고 Chroma 벡터 스토어를 사용할 것입니다.\n",
    "\n",
    "먼저 다음의 과정을 통해 간단한 인덱싱 파이프라인과 RAG 체인을 약 20줄의 코드로 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ed3df",
   "metadata": {},
   "source": [
    "라이브러리\n",
    "\n",
    "- `bs4`는 웹 페이지를 파싱하기 위한 라이브러리입니다.\n",
    "- `langchain`은 AI와 관련된 다양한 기능을 제공하는 라이브러리로, 여기서는 특히 텍스트 분할(`RecursiveCharacterTextSplitter`), 문서 로딩(`WebBaseLoader`), 벡터 저장(`Chroma`, `FAISS`), 출력 파싱(`StrOutputParser`), 실행 가능한 패스스루(`RunnablePassthrough`) 등을 다룹니다.\n",
    "- `langchain_openai` 모듈을 통해 OpenAI의 챗봇(`ChatOpenAI`)과 임베딩(`OpenAIEmbeddings`) 기능을 사용할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d1b0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595217a5",
   "metadata": {},
   "source": [
    "웹 페이지의 내용을 로드하고, 텍스트를 청크로 나누어 인덱싱하는 과정을 거친 후, 관련된 텍스트 스니펫을 검색하여 새로운 내용을 생성하는 과정을 구현합니다.\n",
    "\n",
    "`WebBaseLoader`는 지정된 웹 페이지에서 필요한 부분만을 파싱하기 위해 `bs4.SoupStrainer`를 사용합니다.\n",
    "\n",
    "[참고]\n",
    "\n",
    "- `bs4.SoupStrainer` 는 편리하게 웹에서 원하는 요소를 가져올 수 있도록 해줍니다.\n",
    "\n",
    "(예시)\n",
    "\n",
    "```python\n",
    "bs4.SoupStrainer(\n",
    "    \"div\",\n",
    "    attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]},\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f69f249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/expression_language/', 'content_type': 'text/html; charset=utf-8', 'title': 'Conceptual guide | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nConceptual guide | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual GuideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (rag)RetrievalRetrieversRunnable interfaceStreamingStructured outputsString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy langchain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityConceptual GuideOn this pageConceptual guide\\nThis guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\nWe recommend that you go through at least one of the Tutorials before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.\\nThe conceptual guide does not cover step-by-step instructions or specific implementation examples ‚Äî those are found in the How-to guides and Tutorials. For detailed reference material, please see the API reference.\\nHigh level‚Äã\\n\\nWhy LangChain?: Overview of the value that LangChain provides.\\nArchitecture: How packages are organized in the LangChain ecosystem.\\n\\nConcepts‚Äã\\n\\nChat models: LLMs exposed via a chat API that process sequences of messages as input and output a message.\\nMessages: The unit of communication in chat models, used to represent model input and output.\\nChat history: A conversation represented as a sequence of messages, alternating between user messages and model responses.\\nTools: A function with an associated schema defining the function\\'s name, description, and the arguments it accepts.\\nTool calling: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\\nStructured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\nMemory: Information about a conversation that is persisted so that it can be used in future conversations.\\nMultimodality: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\nRunnable interface: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\nLangChain Expression Language (LCEL): A syntax for orchestrating LangChain components. Most useful for simpler applications.\\nDocument loaders: Load a source as a list of documents.\\nRetrieval: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\\nText splitters: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\\nEmbedding models: Models that represent data such as text or images in a vector space.\\nVector stores: Storage of and efficient search over vectors and associated metadata.\\nRetriever: A component that returns relevant documents from a knowledge base in response to a query.\\nRetrieval Augmented Generation (RAG): A technique that enhances language models by combining them with external knowledge bases.\\nAgents: Use a language model to choose a sequence of actions to take. Agents can interact with external resources via tool.\\nPrompt templates: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\\nOutput parsers: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of tool calling and structured outputs.\\nFew-shot prompting: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\\nExample selectors: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\\nAsync programming: The basics that one should know to use LangChain in an asynchronous context.\\nCallbacks: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\nTracing: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.\\nEvaluation: The process of assessing the performance and effectiveness of AI applications. This involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.\\n\\nGlossary‚Äã\\n\\nAIMessageChunk: A partial response from an AI message. Used when streaming responses from a chat model.\\nAIMessage: Represents a complete response from an AI model.\\nastream_events: Stream granular information from LCEL chains.\\nBaseTool: The base class for all tools in LangChain.\\nbatch: Use to execute a runnable with batch inputs a Runnable.\\nbind_tools: Allows models to interact with tools.\\nCaching: Storing results to avoid redundant calls to a chat model.\\nChat models: Chat models that handle multiple data modalities.\\nConfigurable runnables: Creating configurable Runnables.\\nContext window: The maximum size of input a chat model can process.\\nConversation patterns: Common patterns in chat interactions.\\nDocument: LangChain\\'s representation of a document.\\nEmbedding models: Models that generate vector embeddings for various data types.\\nHumanMessage: Represents a message from a human user.\\nInjectedState: A state injected into a tool function.\\nInjectedStore: A store that can be injected into a tool for data persistence.\\nInjectedToolArg: Mechanism to inject arguments into tool functions.\\ninput and output types: Types used for input and output in Runnables.\\nIntegration packages: Third-party packages that integrate with LangChain.\\ninvoke: A standard method to invoke a Runnable.\\nJSON mode: Returning responses in JSON format.\\nlangchain-community: Community-driven components for LangChain.\\nlangchain-core: Core langchain package. Includes base interfaces and in-memory implementations.\\nlangchain: A package for higher level components (e.g., some pre-built chains).\\nlanggraph: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.\\nlangserve: Use to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\\nManaging chat history: Techniques to maintain and manage the chat history.\\nOpenAI format: OpenAI\\'s message format for chat models.\\nPropagation of RunnableConfig: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\\nrate-limiting: Client side rate limiting for chat models.\\nRemoveMessage: An abstraction used to remove a message from chat history, used primarily in LangGraph.\\nrole: Represents the role (e.g., user, assistant) of a chat message.\\nRunnableConfig: Use to pass run time information to Runnables (e.g., run_name, run_id, tags, metadata, max_concurrency, recursion_limit, configurable).\\nStandard parameters for chat models: Parameters such as API key, temperature, and max_tokens,\\nstream: Use to stream output from a Runnable or a graph.\\nTokenization: The process of converting data into tokens and vice versa.\\nTokens: The basic unit that a language model reads, processes, and generates under the hood.\\nTool artifacts: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\\nTool binding: Binding tools to models.\\n@tool: Decorator for creating tools in LangChain.\\nToolkits: A collection of tools that can be used together.\\nToolMessage: Represents a message that contains the results of a tool execution.\\nVector stores: Datastores specialized for storing and efficiently searching vector embeddings.\\nwith_structured_output: A helper method for chat models that natively support tool calling to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\\nwith_types: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.\\nEdit this pageWas this page helpful?PreviousHow to create and query vector storesNextAgentsHigh levelConceptsGlossaryCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 뉴스기사 내용을 로드하고, 청크로 나누고, 인덱싱합니다.\n",
    "# loader = WebBaseLoader(\n",
    "#     web_paths=(\"https://n.news.naver.com/article/437/0000378416\",),\n",
    "#     bs_kwargs=dict(\n",
    "#         parse_only=bs4.SoupStrainer(\n",
    "#             \"div\",\n",
    "#             attrs={\"class\": [\"newsct_article _article_body\", \"media_end_head_title\"]},\n",
    "#         )\n",
    "#     ),\n",
    "# )\n",
    "\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# LCEL 문서 로드 (내용이 많은 문서)\n",
    "url = \"https://python.langchain.com/docs/expression_language/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=20, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"문서의 수: {len(docs)}\")\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d5bce",
   "metadata": {},
   "source": [
    "`RecursiveCharacterTextSplitter`는 문서를 지정된 크기의 청크로 나눕니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9bf670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=100)\n",
    "\n",
    "# Loader에서 자동으로 넣어준 metadata를 삭제\n",
    "docs[0].metadata = {}\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a0d188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Conceptual guide | ü¶úÔ∏èüîó LangChain'\n",
      "page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval'\n",
      "page_content='a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model'\n",
      "page_content='add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow'\n",
      "page_content='to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge'\n",
      "page_content='to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with'\n",
      "page_content='to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by'\n",
      "page_content='constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow'\n",
      "page_content='a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select'\n",
      "page_content='to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain'\n",
      "page_content='filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple'\n",
      "page_content='historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG'\n",
      "page_content='to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to'\n",
      "page_content='text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call'\n",
      "page_content='vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind'\n",
      "page_content='to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual GuideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation'\n",
      "page_content='Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (rag)RetrievalRetrieversRunnable interfaceStreamingStructured outputsString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy langchain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from'\n",
      "page_content='from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with'\n",
      "page_content='to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityConceptual GuideOn this pageConceptual guide'\n",
      "page_content='This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\n",
      "We recommend that you go through at least one of the Tutorials before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.'\n",
      "page_content='The conceptual guide does not cover step-by-step instructions or specific implementation examples ‚Äî those are found in the How-to guides and Tutorials. For detailed reference material, please see the API reference.\n",
      "High level‚Äã'\n",
      "page_content='Why LangChain?: Overview of the value that LangChain provides.\n",
      "Architecture: How packages are organized in the LangChain ecosystem.\n",
      "\n",
      "Concepts‚Äã'\n",
      "page_content='Chat models: LLMs exposed via a chat API that process sequences of messages as input and output a message.\n",
      "Messages: The unit of communication in chat models, used to represent model input and output.\n",
      "Chat history: A conversation represented as a sequence of messages, alternating between user messages and model responses.\n",
      "Tools: A function with an associated schema defining the function's name, description, and the arguments it accepts.'\n",
      "page_content='Tool calling: A type of chat model API that accepts tool schemas, along with messages, as input and returns invocations of those tools as part of the output message.\n",
      "Structured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\n",
      "Memory: Information about a conversation that is persisted so that it can be used in future conversations.'\n",
      "page_content='Multimodality: The ability to work with data that comes in different forms, such as text, audio, images, and video.\n",
      "Runnable interface: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\n",
      "LangChain Expression Language (LCEL): A syntax for orchestrating LangChain components. Most useful for simpler applications.\n",
      "Document loaders: Load a source as a list of documents.'\n",
      "page_content='Document loaders: Load a source as a list of documents.\n",
      "Retrieval: Information retrieval systems can retrieve structured or unstructured data from a datasource in response to a query.\n",
      "Text splitters: Split long text into smaller chunks that can be individually indexed to enable granular retrieval.\n",
      "Embedding models: Models that represent data such as text or images in a vector space.\n",
      "Vector stores: Storage of and efficient search over vectors and associated metadata.'\n",
      "page_content='Vector stores: Storage of and efficient search over vectors and associated metadata.\n",
      "Retriever: A component that returns relevant documents from a knowledge base in response to a query.\n",
      "Retrieval Augmented Generation (RAG): A technique that enhances language models by combining them with external knowledge bases.\n",
      "Agents: Use a language model to choose a sequence of actions to take. Agents can interact with external resources via tool.'\n",
      "page_content='Prompt templates: Component for factoring out the static parts of a model \"prompt\" (usually a sequence of messages). Useful for serializing, versioning, and reusing these static parts.\n",
      "Output parsers: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily useful prior to the general availability of tool calling and structured outputs.'\n",
      "page_content='Few-shot prompting: A technique for improving model performance by providing a few examples of the task to perform in the prompt.\n",
      "Example selectors: Used to select the most relevant examples from a dataset based on a given input. Example selectors are used in few-shot prompting to select examples for a prompt.\n",
      "Async programming: The basics that one should know to use LangChain in an asynchronous context.'\n",
      "page_content='Async programming: The basics that one should know to use LangChain in an asynchronous context.\n",
      "Callbacks: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\n",
      "Tracing: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.'\n",
      "page_content='Evaluation: The process of assessing the performance and effectiveness of AI applications. This involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.'\n",
      "page_content='Glossary‚Äã'\n",
      "page_content='AIMessageChunk: A partial response from an AI message. Used when streaming responses from a chat model.\n",
      "AIMessage: Represents a complete response from an AI model.\n",
      "astream_events: Stream granular information from LCEL chains.\n",
      "BaseTool: The base class for all tools in LangChain.\n",
      "batch: Use to execute a runnable with batch inputs a Runnable.\n",
      "bind_tools: Allows models to interact with tools.\n",
      "Caching: Storing results to avoid redundant calls to a chat model.'\n",
      "page_content='Caching: Storing results to avoid redundant calls to a chat model.\n",
      "Chat models: Chat models that handle multiple data modalities.\n",
      "Configurable runnables: Creating configurable Runnables.\n",
      "Context window: The maximum size of input a chat model can process.\n",
      "Conversation patterns: Common patterns in chat interactions.\n",
      "Document: LangChain's representation of a document.\n",
      "Embedding models: Models that generate vector embeddings for various data types.\n",
      "HumanMessage: Represents a message from a human user.'\n",
      "page_content='HumanMessage: Represents a message from a human user.\n",
      "InjectedState: A state injected into a tool function.\n",
      "InjectedStore: A store that can be injected into a tool for data persistence.\n",
      "InjectedToolArg: Mechanism to inject arguments into tool functions.\n",
      "input and output types: Types used for input and output in Runnables.\n",
      "Integration packages: Third-party packages that integrate with LangChain.\n",
      "invoke: A standard method to invoke a Runnable.\n",
      "JSON mode: Returning responses in JSON format.'\n",
      "page_content='invoke: A standard method to invoke a Runnable.\n",
      "JSON mode: Returning responses in JSON format.\n",
      "langchain-community: Community-driven components for LangChain.\n",
      "langchain-core: Core langchain package. Includes base interfaces and in-memory implementations.\n",
      "langchain: A package for higher level components (e.g., some pre-built chains).\n",
      "langgraph: Powerful orchestration layer for LangChain. Use to build complex pipelines and workflows.'\n",
      "page_content='langserve: Use to deploy LangChain Runnables as REST endpoints. Uses FastAPI. Works primarily for LangChain Runnables, does not currently integrate with LangGraph.\n",
      "Managing chat history: Techniques to maintain and manage the chat history.\n",
      "OpenAI format: OpenAI's message format for chat models.\n",
      "Propagation of RunnableConfig: Propagating configuration through Runnables. Read if working with python 3.9, 3.10 and async.\n",
      "rate-limiting: Client side rate limiting for chat models.'\n",
      "page_content='rate-limiting: Client side rate limiting for chat models.\n",
      "RemoveMessage: An abstraction used to remove a message from chat history, used primarily in LangGraph.\n",
      "role: Represents the role (e.g., user, assistant) of a chat message.\n",
      "RunnableConfig: Use to pass run time information to Runnables (e.g., run_name, run_id, tags, metadata, max_concurrency, recursion_limit, configurable).\n",
      "Standard parameters for chat models: Parameters such as API key, temperature, and max_tokens,'\n",
      "page_content='Standard parameters for chat models: Parameters such as API key, temperature, and max_tokens,\n",
      "stream: Use to stream output from a Runnable or a graph.\n",
      "Tokenization: The process of converting data into tokens and vice versa.\n",
      "Tokens: The basic unit that a language model reads, processes, and generates under the hood.\n",
      "Tool artifacts: Add artifacts to the output of a tool that will not be sent to the model, but will be available for downstream processing.\n",
      "Tool binding: Binding tools to models.'\n",
      "page_content='Tool binding: Binding tools to models.\n",
      "@tool: Decorator for creating tools in LangChain.\n",
      "Toolkits: A collection of tools that can be used together.\n",
      "ToolMessage: Represents a message that contains the results of a tool execution.\n",
      "Vector stores: Datastores specialized for storing and efficiently searching vector embeddings.'\n",
      "page_content='Vector stores: Datastores specialized for storing and efficiently searching vector embeddings.\n",
      "with_structured_output: A helper method for chat models that natively support tool calling to get structured output matching a given schema specified via Pydantic, JSON schema or a function.\n",
      "with_types: Method to overwrite the input and output types of a runnable. Useful when working with complex LCEL chains and deploying with LangServe.'\n",
      "page_content='Edit this pageWas this page helpful?PreviousHow to create and query vector storesNextAgentsHigh levelConceptsGlossaryCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'\n"
     ]
    }
   ],
   "source": [
    "for i, split in enumerate(splits):\n",
    "    # 검색된 chunk 구별을 위해 metadata에 id값을 삽입\n",
    "    split.metadata[\"id\"] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a38783",
   "metadata": {},
   "source": [
    "`FAISS` 혹은 `Chroma`와 같은 vectorstore는 이러한 청크를 바탕으로 문서의 벡터 표현을 생성합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a8ca04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcw\\AppData\\Local\\Temp\\ipykernel_20928\\2272273826.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 벡터스토어를 생성합니다.\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=OpenAIEmbeddings(), \n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}    # cosine 유사도 알고리즘을 사용합니다.\n",
    "    )\n",
    "\n",
    "# 문서를 저장합니다.\n",
    "vector_ids = vectorstore.add_documents(splits)\n",
    "len(vector_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d79a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시 질의와 관련된 문서를 검색합니다.\n",
    "query_example = \"LangChain Expression Language (LCEL)는 무엇을 위해 설계되었나요?\"\n",
    "references_k_4 = vectorstore.similarity_search_with_score(query=query_example, k=4)\n",
    "references_k_10 = vectorstore.similarity_search_with_score(query=query_example, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09b35306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'id': 1}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval'),\n",
       "  0.20861202478408813),\n",
       " (Document(metadata={'id': 0}, page_content='Conceptual guide | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain'),\n",
       "  0.19854122400283813),\n",
       " (Document(metadata={'id': 16}, page_content='Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (rag)RetrievalRetrieversRunnable interfaceStreamingStructured outputsString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy langchain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from'),\n",
       "  0.19195681810379028),\n",
       " (Document(metadata={'id': 24}, page_content='Multimodality: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\nRunnable interface: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\nLangChain Expression Language (LCEL): A syntax for orchestrating LangChain components. Most useful for simpler applications.\\nDocument loaders: Load a source as a list of documents.'),\n",
       "  0.18360483646392822)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references_k_4.sort(key=lambda x:x[1], reverse=True)\n",
    "references_k_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2b3d06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'id': 19}, page_content='This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly.\\nWe recommend that you go through at least one of the Tutorials before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here.'),\n",
       "  0.23369503021240234),\n",
       " (Document(metadata={'id': 41}, page_content='Edit this pageWas this page helpful?PreviousHow to create and query vector storesNextAgentsHigh levelConceptsGlossaryCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.'),\n",
       "  0.23236584663391113),\n",
       " (Document(metadata={'id': 10}, page_content='filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple'),\n",
       "  0.23157453536987305),\n",
       " (Document(metadata={'id': 15}, page_content='to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual GuideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation'),\n",
       "  0.22350209951400757),\n",
       " (Document(metadata={'id': 21}, page_content='Why LangChain?: Overview of the value that LangChain provides.\\nArchitecture: How packages are organized in the LangChain ecosystem.\\n\\nConcepts‚Äã'),\n",
       "  0.21622121334075928),\n",
       " (Document(metadata={'id': 29}, page_content='Async programming: The basics that one should know to use LangChain in an asynchronous context.\\nCallbacks: Callbacks enable the execution of custom auxiliary code in built-in components. Callbacks are used to stream outputs from LLMs in LangChain, trace the intermediate steps of an application, and more.\\nTracing: The process of recording the steps that an application takes to go from input to output. Tracing is essential for debugging and diagnosing issues in complex applications.'),\n",
       "  0.2160109281539917),\n",
       " (Document(metadata={'id': 1}, page_content='Skip to main contentIntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval'),\n",
       "  0.20861202478408813),\n",
       " (Document(metadata={'id': 0}, page_content='Conceptual guide | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain'),\n",
       "  0.19854122400283813),\n",
       " (Document(metadata={'id': 16}, page_content='Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (rag)RetrievalRetrieversRunnable interfaceStreamingStructured outputsString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy langchain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from'),\n",
       "  0.19195681810379028),\n",
       " (Document(metadata={'id': 24}, page_content='Multimodality: The ability to work with data that comes in different forms, such as text, audio, images, and video.\\nRunnable interface: The base abstraction that many LangChain components and the LangChain Expression Language are built on.\\nLangChain Expression Language (LCEL): A syntax for orchestrating LangChain components. Most useful for simpler applications.\\nDocument loaders: Load a source as a list of documents.'),\n",
       "  0.18360483646392822)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references_k_10.sort(key=lambda x:x[1], reverse=True)\n",
    "references_k_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a89f55",
   "metadata": {},
   "source": [
    "`vectorstore.as_retriever()`를 통해 생성된 검색기는 `hub.pull`로 가져온 프롬프트와 `ChatOpenAI` 모델을 사용하여 새로운 내용을 생성합니다.\n",
    "\n",
    "마지막으로, `StrOutputParser`는 생성된 결과를 문자열로 파싱합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\n",
    "검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\n",
    "한글로 답변해 주세요. 단, 기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요.\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35016e46",
   "metadata": {},
   "source": [
    "hub 에서 `teddynote/rag-prompt-korean` 프롬프트를 다운로드 받아 입력할 수 있습니다. 이런 경우 별도의 프롬프트 작성과정이 생략됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = hub.pull(\"teddynote/rag-prompt-korean\")\n",
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "# 체인을 생성합니다.\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9370eb",
   "metadata": {},
   "source": [
    "스트리밍 출력을 위하여 `stream_response` 를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fed977",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948a029",
   "metadata": {},
   "source": [
    "> [LangSmith Trace 보기](https://smith.langchain.com/public/c6047a61-8f44-48e5-89eb-b1e8a6321cea/r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78275b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_chain.stream(\"부영그룹의 출산 장려 정책에 대해 설명해주세요.\")\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eec33e",
   "metadata": {},
   "source": [
    "> [LangSmith Trace 보기](https://smith.langchain.com/public/ed21d80e-b4da-4a08-823b-ed980db9c347/r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c96f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_chain.stream(\"부영그룹은 출산 직원에게 얼마의 지원을 제공하나요?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044ceba",
   "metadata": {},
   "source": [
    "> [LangSmith Trace 보기](https://smith.langchain.com/public/df80c528-61d6-4c83-986a-3373a4039dae/r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_chain.stream(\"정부의 저출생 대책을 bullet points 형식으로 작성해 주세요.\")\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c627d96",
   "metadata": {},
   "source": [
    "> [LangSmith Trace 보기](https://smith.langchain.com/public/1a613ee7-6eaa-482f-a45f-8c22b4e60fbf/r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_chain.stream(\"부영그룹의 임직원 숫자는 몇명인가요?\")\n",
    "stream_response(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
